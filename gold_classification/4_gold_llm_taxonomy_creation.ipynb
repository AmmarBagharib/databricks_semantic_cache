{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be61a462-e7a6-4654-9b11-d540ea290b34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !pip install databricks_langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd274ae4-788c-4da5-b710-99d602e54556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install --force-reinstall typing-extensions\n",
    "# %restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10179c09-0eb8-4cb9-a442-bc7ed69d6035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import json\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "584ff972-9c0f-470a-969a-8deaac3efae4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_gold_summarized = spark.table(\"gold.conversations_summarized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5609380-8b4a-4c80-8f58-1cc74939378c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CREATE TAXONOMY USING LLM\n",
    "DATABRICKS_ENDPOINT = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "llm = ChatDatabricks(endpoint=DATABRICKS_ENDPOINT, temperature=0.0)\n",
    "\n",
    "CLASSIFICATION_SAMPLE_LIMIT = 5000 # 10% of dataset\n",
    "df_gold_sampled = (\n",
    "    df_gold_summarized\n",
    "    .orderBy(F.rand())\n",
    "    .limit(CLASSIFICATION_SAMPLE_LIMIT)\n",
    ")\n",
    "\n",
    "# Prepare summaries as a single concatenated string\n",
    "summaries_concat = df_gold_sampled.select(\n",
    "    F.concat_ws(\"\\n\\n\", F.collect_list(\"convo_summary\")).alias(\"all_summaries\")\n",
    ").limit(1)\n",
    "\n",
    "summaries_str = summaries_concat.collect()[0]['all_summaries']\n",
    "print(len(summaries_str))\n",
    "\n",
    "sys_prompt = \"\"\"\n",
    "You are a classification assistant. INPUT: a single text block containing 5000+ short summaries, each separated by a blank line. TASK: read **all** summaries in full before responding, and then generate a final list (array) of **no more than 15** broad, meaningful, non-overlapping categories that represent the themes present across all summaries.\n",
    "\n",
    "REQUIREMENTS:\n",
    "1. Do not output anything until you have processed the entire input.\n",
    "2. Produce between 1 and 15 categories, using fewer if appropriate.\n",
    "3. Categories must be:\n",
    "   • Distinct and non-overlapping  \n",
    "   • Broad and meaningful (avoid hyper-specific labels)  \n",
    "   • Representative of major themes across the summaries  \n",
    "4. The **output format must be exactly**:\n",
    "\n",
    "   [\"category_one\", \"category_two\", \"category_three\", ...]\n",
    "\n",
    "5. Category strings must be:\n",
    "   • lowercase_snake_case  \n",
    "   • 1–5 words  \n",
    "   • concise, thematic, and mutually exclusive  \n",
    "6. Internal reasoning steps:\n",
    "   a. Scan all summaries and identify recurring topics.  \n",
    "   b. Draft 10–25 candidate themes.  \n",
    "   c. Merge overlapping themes; eliminate categories that are too narrow.  \n",
    "   d. Validate that all summaries can reasonably fit under one of the final categories.  \n",
    "   e. Limit the final categories to a max of 15.  \n",
    "7. Do **not** provide explanations, descriptions, or examples.  \n",
    "8. Output only the final array. No commentary.\n",
    "\n",
    "Now read the provided summaries and output the final category list.\n",
    "\"\"\".strip()\n",
    "        \n",
    "msgs = [\n",
    "    {\"role\": \"system\", \"content\": sys_prompt},\n",
    "    {\"role\": \"user\", \"content\": summaries_str},\n",
    "]\n",
    "\n",
    "response = llm.invoke(msgs).content\n",
    "        \n",
    "# Parse string representation of list into actual list\n",
    "try:\n",
    "    # Try JSON parsing first (cleanest)\n",
    "    categories_list = json.loads(response)\n",
    "except json.JSONDecodeError:\n",
    "    try:\n",
    "        # Fall back to ast.literal_eval for Python list format\n",
    "        categories_list = ast.literal_eval(response)\n",
    "    except (ValueError, SyntaxError):\n",
    "        # If both fail, try to extract list manually\n",
    "        # Strip any surrounding text and get content between brackets\n",
    "        start_idx = response.find('[')\n",
    "        end_idx = response.rfind(']')\n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            list_str = response[start_idx:end_idx+1]\n",
    "            categories_list = ast.literal_eval(list_str)\n",
    "        else:\n",
    "            # Return empty list if parsing completely fails\n",
    "            categories_list = []\n",
    "print(len(categories_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5912be5a-1060-4f6d-8986-4c4b96736470",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save categories_list to JSON file"
    }
   },
   "outputs": [],
   "source": [
    "with open('/Workspace/Users/ammarbagharib@gmail.com/categories_list.json', 'w') as f:\n",
    "    json.dump(categories_list, f)\n",
    "print('categories_list saved to /Workspace/Users/ammarbagharib@gmail.com/categories_list.json')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "gold_llm_taxonomy_creation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}